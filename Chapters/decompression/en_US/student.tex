\chapter{Data Compression and Decompression}

Data compression and decompression are fundamental techniques used in
modern computing, enabling efficient storage and transmission of
data. The concept of entropy, borrowed from the field of information
theory, plays a crucial role in determining the compression rate.\index{data compression}

\section{Data Compression and Decompression}

Data compression is the process of reducing the amount of data needed
to represent a particular set of information. The two main types of
data compression are lossless and lossy. Lossless compression ensures
that the original data can be perfectly reconstructed from the
compressed data, whereas lossy compression allows some loss of data
for more significant compression rates.

Decompression is the reverse process of compression, reconstructing
the original data from the compressed format.

\section{Entropy}

In information theory, entropy measures the unpredictability or
randomness of information content. More specifically, it quantifies
the expected value of the information contained in a message. Lower
entropy implies less randomness and more repetitiveness, which in turn
means the data can be compressed more.\index{entropy}

\section{Entropy and Compression}

The role of entropy in data compression is fundamental. The entropy of
a source of data is the minimum number of bits required, on average,
to encode symbols drawn from the source. It serves as a lower bound on
the best possible lossless compression rate.

For a source $X$ with probability distribution $p(x)$, the entropy $H(X)$ is defined as:

\begin{equation}
H(X) = - \sum_{x \in X} p(x) \log_2 p(x)
\end{equation}

If the entropy of the data is high (i.e., the data is random and
unpredictable), the potential for compression is low. On the other
hand, if the entropy is low (the data is predictable), the data can be
compressed to a smaller size.

