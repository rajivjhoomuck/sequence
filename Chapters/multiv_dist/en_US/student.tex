\chapter{Multivariate Distributions}

The world of probability and statistics doesn't limit itself to the
study of single variables. Often, we are interested in the
interconnections, relationships, and associations among several
variables. In such a scenario, the univariate distributions that we
have studied so far become inadequate. To comprehend the joint
behavior of these variables and to uncover the underlying patterns of
dependency, we must turn to the realm of multivariate distributions.

This chapter aims to introduce the reader to the concept of
multivariate probability distributions. These are probability
distributions that take into account and describe the behavior of more
than one random variable. We will start our exploration with a
discussion on the joint probability mass and density functions. These
functions extend the concepts of probability mass and density
functions for one variable to the situation where we have multiple
variables.

Next, we will explore important properties of joint distributions,
including the concept of marginal distribution and conditional
distribution, which allow us to explore the probability of a subset of
variables while conditioning on, or ignoring, other variables. We will
also introduce the idea of independence of random variables in the
multivariate context.

Subsequently, we will discuss some commonly used multivariate
distributions such as the multivariate normal distribution, and the
multivariate Bernoulli and binomial distributions. These specific
distributions will provide us with practical tools for modelling
multivariate data.

Finally, we will delve into covariance and correlation, two key
measures that give us a sense of how two variables change
together. Understanding these measures is critical for capturing the
relationships in multivariate data.


