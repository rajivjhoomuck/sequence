\chapter{Multiple Logistic Regression}

The simple logistic regression model, discussed in the last chapter,
uses only one predictor variable, while multiple logistic regression,
as the name implies, allows for more than one predictor variable.

\section{Multiple Logistic Regression}

In multiple logistic regression, we want to model the relationship
between a binary response variable and multiple predictor
variables. Let $y$ be the binary response variable and $x_1, x_2, ...,
x_p$ be $p$ predictor variables. The multiple logistic regression
model has the form:

\begin{equation*}
\ln \left( \frac{P(Y=1|X)}{1-P(Y=1|X)} \right) = \beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p
\end{equation*}

where $P(Y=1|X)$ is the probability of the event $Y=1$ given the
predictor variables, and $\beta_0, \beta_1, ..., \beta_p$ are the
parameters of the model. This equation can also be rewritten in terms
of the probability $P(Y=1|X)$:

\begin{equation*}
P(Y=1|X) = \frac{e^{\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p}}{1 + e^{\beta_0 + \beta_1X_1 + \beta_2X_2 + ... + \beta_pX_p}}
\end{equation*}

In this model, each one-unit increase in $X_i$ multiplies the odds of
$Y=1$ by $e^{\beta_i}$, holding all other predictors constant.

\section{Divide by 4 Rule}

The "Divide by 4" rule is a rule of thumb for interpreting the
coefficients in logistic regression. It says that for small values of
$\beta_i$, a one-unit increase in $X_i$ will change the probability
$P(Y=1|X)$ by approximately $\beta_i / 4$ at the average value of
$X_i$.

The rule arises from the derivative of the logistic function at its
midpoint, and provides a useful and simple way to get an approximate
sense of the effect size when interpreting the coefficients.
